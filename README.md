# ğŸ§  Neural Networks and Deep Learning â€” Study Notes

This repository contains **concise notes, mathematical proofs, experiments, and solved exercises** based on
ğŸ“˜ *Neural Networks and Deep Learning* by Michael Nielsen.

The focus is on **conceptual clarity**, **mathematical correctness**, and **bridging theory with implementation**, rather than framework-specific code.

ğŸ”— Official book reference: [http://neuralnetworksanddeeplearning.com/](http://neuralnetworksanddeeplearning.com/)

---

## ğŸ“‚ Chapter 1 â€” Neural Network Foundations

This chapter builds the intuition behind neural networks, starting from perceptrons and moving toward differentiable models and optimization.

### Covered Concepts

* ğŸ“Š Binary outputs and probabilistic interpretation
* ğŸ” Online learning vs. mini-batch learning
* ğŸ”€ Sigmoid neurons as smooth approximations to perceptrons
* ğŸ“‰ One-dimensional gradient descent experiments

### Key Takeaway

Establishes why **differentiability and gradient-based learning** are central to neural networks.

---

## ğŸ“‚ Chapter 2 â€” Backpropagation Theory

This chapter focuses exclusively on the **mathematical derivation of backpropagation**, independent of code.

### Covered Concepts

* ğŸ§® Vectorized backpropagation equations
* âœï¸ Formal proofs of error propagation across layers

### Key Takeaway

Provides a rigorous understanding of **how and why gradients flow backward** in neural networks.

---

## ğŸ“‚ Chapter 3 â€” Improving Learning (Solved Exercises)

This chapter contains fully solved theoretical exercises focused on optimization and cost functions.

### Solved Topics

* ğŸ“ Analytical verification of the sigmoid derivative
* âš ï¸ Common pitfalls in cross-entropy formulations
* ğŸ“‰ Proof that cross-entropy is minimized when predictions match targets, including soft-label cases
* ğŸ”¢ Interpretation of the resulting minimum as binary entropy

### Key Takeaway

Clarifies why **cross-entropy is the preferred loss function** for probabilistic models.

---

## ğŸ¯ Repository Goals

* ğŸ“˜ Deepen understanding of neural network fundamentals
* âœï¸ Practice mathematical reasoning behind learning algorithms
* ğŸ§ª Validate theory through minimal experiments
* ğŸ§  Build strong foundations before using high-level frameworks
